# -*- coding: utf-8 -*-
import os
import json
import dashscope
import numpy as np
from dashscope import Generation
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings

# ===================== é…ç½®åŒº =====================
CONFIG = {
    "ali_api_key": os.getenv("ALI_API_KEY"),       # ä»ç¯å¢ƒå˜é‡è·å–é˜¿é‡Œäº‘API Key
    "llm_model": "qwen-turbo",                     # å¯é€‰qwen-turbo/qwen-plus/qwen-max
    "chunk_size": 500,                            # æ›´å°çš„åˆ†å—å¤§å°
    "chunk_overlap": 50,
    "top_k": 3,                                   # å‡å°‘è¿”å›æ–‡æ¡£æ•°é‡
}

# è®¾ç½®é˜¿é‡Œäº‘SDK
dashscope.api_key = CONFIG["ali_api_key"]

# ===================== è‡ªå®šä¹‰åµŒå…¥æ¨¡å— =====================
class CustomEmbeddings(Embeddings):
    """ç¬¦åˆLangchainæ¥å£çš„è‡ªå®šä¹‰åµŒå…¥å®ç°"""
    def __init__(self):
        super().__init__()
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„è™šæ‹ŸåµŒå…¥æ¨¡å‹
        self.embedding_size = 128  # æ›´å°çš„åµŒå…¥ç»´åº¦
        self.vocab = {}
        self.next_id = 1
        self.embeddings_cache = {}

    def embed_text(self, text):
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        # å¦‚æœå·²ç»ç¼“å­˜è¿‡ï¼Œç›´æ¥è¿”å›
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]

        # ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºIDå¹¶ç”ŸæˆéšæœºåµŒå…¥
        words = text.split()
        word_ids = []
        for word in words:
            if word not in self.vocab:
                self.vocab[word] = self.next_id
                self.next_id += 1
            word_ids.append(self.vocab[word])

        # ç”Ÿæˆä¸€ä¸ªç®€å•çš„å¹³å‡åµŒå…¥å‘é‡
        embeddings = np.zeros(self.embedding_size)
        for word_id in word_ids:
            np.random.seed(word_id)  # ä¿è¯ä¸€è‡´æ€§
            embeddings += np.random.randn(self.embedding_size)
        embeddings /= len(word_ids)

        # å½’ä¸€åŒ–å‘é‡
        norm = np.linalg.norm(embeddings)
        if norm > 0:
            embeddings /= norm

        # ç¼“å­˜ç»“æœ
        self.embeddings_cache[text] = embeddings
        return embeddings

    def embed_documents(self, texts):
        """å®ç°Langchainæ‰€éœ€çš„æ¥å£æ–¹æ³• - æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        return [self.embed_text(text) for text in texts]

    def embed_query(self, text):
        """å®ç°Langchainæ‰€éœ€çš„æ¥å£æ–¹æ³• - åµŒå…¥æŸ¥è¯¢"""
        return self.embed_text(text)

# ===================== 1. çŸ¥è¯†åº“æ„å»ºæ¨¡å— =====================
class KnowledgeBaseBuilder:
    """æ•…éšœçŸ¥è¯†åº“æ„å»ºå™¨ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    def __init__(self, config):
        self.config = config
        # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹
        self.embedder = CustomEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"]
        )

    def process_data(self, historical_data):
        """æ•°æ®å¤„ç†ï¼šåˆ†å—å’Œå‘é‡åŒ–"""
        documents = []
        metadatas = []

        for record in historical_data:
            # æ„å»ºæ–‡æ¡£æè¿° = ç°è±¡ + åˆ†ç±» + æªæ–½ 
            text = f"æ•…éšœç°è±¡: {record['symptom']}\nåˆ†ç±»: {record['category']}\næªæ–½: {record['solution']}"
            chunks = self.text_splitter.split_text(text)
            
            for chunk in chunks:
                documents.append(chunk)
                metadatas.append({
                    "source": record.get("source", "unknown"),
                    "timestamp": record.get("timestamp", datetime.now().isoformat()),
                    "category": record["category"]
                })

        # åˆ›å»ºå‘é‡æ•°æ®åº“ 
        print(f"åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")
        vector_db = Chroma.from_texts(
            documents, 
            metadatas=metadatas,
            embedding=self.embedder
        )
        return vector_db, documents

# ===================== 2. ç®€åŒ–æ£€ç´¢æ¨¡å— =====================
class SimplifiedRetriever:
    """ç®€åŒ–çš„æ··åˆæ£€ç´¢å™¨"""
    def __init__(self, vector_db, documents, config):
        self.config = config
        self.vector_db = vector_db
        self.documents = documents

        # åˆ›å»ºBM25æ£€ç´¢å™¨
        print("åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")
        self.bm25_retriever = BM25Retriever.from_texts(documents)
        self.bm25_retriever.k = config["top_k"]

    def retrieve(self, query):
        """æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # å‘é‡æ£€ç´¢
        try:
            vector_results = self.vector_db.similarity_search(query, k=self.config["top_k"])
            vector_contents = [doc.page_content for doc in vector_results]
        except Exception as e:
            print(f"å‘é‡æ£€ç´¢é”™è¯¯: {e}")
            vector_contents = []

        # BM25æ£€ç´¢
        try:
            bm25_results = self.bm25_retriever.get_relevant_documents(query)
            bm25_contents = [doc.page_content for doc in bm25_results]
        except Exception as e:
            print(f"BM25æ£€ç´¢é”™è¯¯: {e}")
            bm25_contents = []

        # èåˆç»“æœå¹¶å»é‡
        all_contents = list(set(vector_contents + bm25_contents))
        print(f"æ£€ç´¢åˆ° {len(all_contents)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")

        # ç®€å•è¿”å›top_k
        return all_contents[:self.config["top_k"]]

# ===================== 3. RAGæ¨ç†æ¨¡å—ï¼ˆé€‚é…é˜¿é‡ŒAPIï¼‰ =====================
class AliyunLLMChain:
    """å°è£…é˜¿é‡Œäº‘å¤§æ¨¡å‹è°ƒç”¨"""
    def __init__(self, config):
        self.config = config
        self.model = config["llm_model"]

    def generate(self, prompt):
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            print("è°ƒç”¨é˜¿é‡Œå¤§æ¨¡å‹API...")
            response = Generation.call(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                result_format='message'
            )
            return response["output"]["choices"][0]["message"]["content"]
        except Exception as e:
            return f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

class FaultDiagnosisSystem:
    """æ•…éšœå®šä½æ ¸å¿ƒç³»ç»Ÿ"""
    def __init__(self, retriever, config):
        self.retriever = retriever
        self.config = config
        self.llm = AliyunLLMChain(config)

        # æ›´ç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿
        self.prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è¿ç»´ä¸“å®¶ï¼Œè¯·æ ¹æ®å‘Šè­¦ä¿¡æ¯å’Œå†å²æ•…éšœæ•°æ®è¯Šæ–­é—®é¢˜ï¼š

        <å½“å‰å‘Šè­¦>
        æè¿°: {alert_desc}
        æŒ‡æ ‡: {alert_metrics}
        æ—¥å¿—: {alert_logs}

        <ç›¸å…³å†å²æ¡ˆä¾‹>
        {context}

        è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºåˆ†æç»“æœï¼š
        1. æ•…éšœç±»åˆ«: ä»å†å²æ¡ˆä¾‹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„åˆ†ç±»
        2. è¯Šæ–­ä¾æ®: ç»“åˆå½“å‰å‘Šè­¦å’Œå†å²æ¡ˆä¾‹ç›¸ä¼¼ç‚¹åˆ†æ
        3. ç´§æ€¥æªæ–½: å»ºè®®çš„ç¬¬ä¸€æ­¥æ“ä½œ
        4. å¤‡æ³¨: ä»»ä½•é¢å¤–æ³¨æ„äº‹é¡¹
        """

    def build_prompt(self, alert_desc, alert_metrics, alert_logs, context):
        """æ„å»ºå®Œæ•´æç¤º"""
        # æ ¼å¼åŒ–å†å²æ¡ˆä¾‹
        context_str = "\n".join([f"ğŸ“ {doc}" for doc in context])
 
        print("\n======= æç¤ºå†…å®¹ =======")
        print(self.prompt_template.format(
            alert_desc=alert_desc,
            alert_metrics=alert_metrics,
            alert_logs=alert_logs,
            context=context_str
        ))
        print("=" * 40)

        return self.prompt_template.format(
            alert_desc=alert_desc,
            alert_metrics=alert_metrics,
            alert_logs=alert_logs,
            context=context_str
        )

    def diagnose(self, alert_desc, alert_metrics, alert_logs):
        """æ‰§è¡Œæ•…éšœè¯Šæ–­"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print(f"æ£€ç´¢å‘Šè­¦: {alert_desc}...")
        context_docs = self.retriever(alert_desc)

        # æ„å»ºæç¤º
        print("æ„å»ºæç¤º...")
        prompt = self.build_prompt(
            alert_desc, 
            alert_metrics, 
            alert_logs,
            context_docs
        )
   
        # è°ƒç”¨é˜¿é‡Œå¤§æ¨¡å‹
        return self.llm.generate(prompt)

# ===================== ä¸»æ‰§è¡Œæµç¨‹ =====================
if __name__ == "__main__":
    print("æ­£åœ¨åˆå§‹åŒ–æ•…éšœè¯Šæ–­ç³»ç»Ÿ...")

    # 0. ç¡®ä¿é˜¿é‡ŒAPIå¯†é’¥å­˜åœ¨
    if not CONFIG["ali_api_key"]:
        print("é”™è¯¯ï¼šæœªè®¾ç½®ALI_API_KEYç¯å¢ƒå˜é‡ï¼")
        print("è¯·æ‰§è¡Œï¼šexport ALI_API_KEY=æ‚¨çš„é˜¿é‡ŒAPIå¯†é’¥")
        exit(1)

    # 1. æ„å»ºçŸ¥è¯†åº“ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰
    historical_data = [
        {
            "symptom": "æ•°æ®åº“å“åº”å»¶è¿Ÿçªå¢è‡³5sï¼ŒCPUä½¿ç”¨ç‡90%",
            "category": "æ•°æ®åº“æ­»é”",
            "solution": "1. æ£€æŸ¥é”ç­‰å¾…é“¾\n2. ç»ˆæ­¢é˜»å¡äº‹åŠ¡\n3. ä¼˜åŒ–ç´¢å¼•"
        },
        {
            "symptom": "å†…å­˜å ç”¨æ¯å°æ—¶å¢é•¿2%ï¼ŒFull GCé¢‘ç¹",
            "category": "å†…å­˜æ³„æ¼",
            "solution": "1. ç”Ÿæˆå †è½¬å‚¨\n2. åˆ†æå†…å­˜å¯¹è±¡\n3. ä¿®å¤å¼•ç”¨é“¾"
        },
        {
            "symptom": "æœåŠ¡æ¥å£æˆåŠŸç‡ä»99.9%éª¤é™è‡³60%ï¼Œé”™è¯¯ç ConfigNotFount",
            "category": "é…ç½®é”™è¯¯",
            "solution": "1. æ£€æŸ¥æœ€è¿‘éƒ¨ç½²çš„é…ç½®\n2. å›æ»šæœ€æ–°é…ç½®\n3. éªŒè¯é…ç½®åŠ è½½æµç¨‹"
        },
        {
            "symptom": "æ¥å£æµé‡è¶…è¿‡é˜ˆå€¼95%ï¼ŒTCPé‡ä¼ ç‡å¢åŠ 300%",
            "category": "ç½‘ç»œé˜»å¡",
            "solution": "1. æ‰©å®¹å¸¦å®½\n2. ä¼˜åŒ–æµé‡è°ƒåº¦\n3. åˆ†æå¼‚å¸¸æµé‡æº"
        }
    ]

    # 1.1 åˆå§‹åŒ–çŸ¥è¯†åº“
    print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
    kb_builder = KnowledgeBaseBuilder(CONFIG)
    vector_db, documents = kb_builder.process_data(historical_data)
    print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    # 2. åˆå§‹åŒ–æ£€ç´¢å™¨
    print("åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨...")
    retriever = SimplifiedRetriever(vector_db, documents, CONFIG)

    # 2.1 åˆ›å»ºæ£€ç´¢å‡½æ•°
    def retrieve_function(query):
        return retriever.retrieve(query)

    # 3. åˆå§‹åŒ–è¯Šæ–­ç³»ç»Ÿ
    print("åˆå§‹åŒ–è¯Šæ–­å¼•æ“...")
    diagnosis_system = FaultDiagnosisSystem(
        retrieve_function, 
        CONFIG
    )
    print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡æ¥å—è¯Šæ–­è¯·æ±‚")
    print("=" * 60)

    # 4. æµ‹è¯•ç”¨ä¾‹ï¼ˆæ¨¡æ‹Ÿæ•…éšœå‘Šè­¦ï¼‰
    test_cases = [
        {
            "desc": "è®¢å•æœåŠ¡å“åº”è¶…æ—¶ç‡è¶…è¿‡60%",
            "metrics": "DBè¿æ¥æ± æ´»è·ƒè¿æ¥100%ï¼ŒSQLæ‰§è¡Œå¹³å‡è€—æ—¶2.5s",
            "logs": "å‘ç°å¤§é‡'Lock wait timeout'é”™è¯¯æ—¥å¿—"
        },
        {
            "desc": "ç”¨æˆ·æœåŠ¡APIå¤±è´¥ç‡çªå¢",
            "metrics": "æœ€è¿‘éƒ¨ç½²åå¤±è´¥ç‡ä»0.1%å‡è‡³40%",
            "logs": "å‡ºç°å¤§é‡'ConfigNotFount'é”™è¯¯"
        },
        {
            "desc": "æœåŠ¡å†…å­˜ä½¿ç”¨æŒç»­å¢é•¿",
            "metrics": "JVMå†…å­˜å ç”¨æ¯å°æ—¶å¢é•¿3%ï¼ŒGCé¢‘ç‡å¢åŠ ",
            "logs": "è§‚å¯Ÿåˆ°é¢‘ç¹çš„Full GCæ—¥å¿—"
        }
    ]

    # 5. æ‰§è¡Œè¯Šæ–­
    for i, test_alert in enumerate(test_cases):
        print(f"\n===== æµ‹è¯•ç”¨ä¾‹ {i+1} =====")
        print(f"å‘Šè­¦æè¿°: {test_alert['desc']}")
        print(f"ç›¸å…³æŒ‡æ ‡: {test_alert['metrics']}")
        print(f"æ—¥å¿—ä¿¡æ¯: {test_alert['logs']}")

        print("\næ­£åœ¨åˆ†æä¸­...")
        # æ‰§è¡Œè¯Šæ–­
        result = diagnosis_system.diagnose(
            test_alert["desc"],
            test_alert["metrics"],
            test_alert["logs"]
        )

        print("\n===== è¯Šæ–­ç»“æœ =====")
        print(result)
        print("=" * 100)

    print("æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå®Œæ¯•")
