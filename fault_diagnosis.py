# -*- coding: utf-8 -*-
import os
import json
import dashscope
import numpy as np
from dashscope import Generation
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings

# å¯¼å…¥é…ç½®æ–‡ä»¶
from config import SYSTEM_CONFIG, HISTORICAL_DATA, TEST_CASES, validate_config

# ===================== é…ç½®åŒº =====================
CONFIG = SYSTEM_CONFIG.copy()
CONFIG["ali_api_key"] = os.getenv("ALI_API_KEY")  # ä»ç¯å¢ƒå˜é‡è·å–é˜¿é‡Œäº‘API Key

# è®¾ç½®é˜¿é‡Œäº‘SDK
dashscope.api_key = CONFIG["ali_api_key"]

# ===================== è‡ªå®šä¹‰åµŒå…¥æ¨¡å— =====================
class CustomEmbeddings(Embeddings):
    """ç¬¦åˆLangchainæ¥å£çš„è‡ªå®šä¹‰åµŒå…¥å®ç°"""
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨æ›´å¤§çš„åµŒå…¥ç»´åº¦ä»¥æé«˜è¡¨è¾¾èƒ½åŠ›
        self.embedding_size = 384  # å¢åŠ ç»´åº¦
        self.vocab = {}
        self.next_id = 1
        self.embeddings_cache = {}

    def embed_text(self, text):
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥ - æ”¹è¿›ç‰ˆæœ¬"""
        # å¦‚æœå·²ç»ç¼“å­˜è¿‡ï¼Œç›´æ¥è¿”å›
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]

        # æ–‡æœ¬é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–å’Œåˆ†è¯
        processed_text = self._preprocess_text(text)
        
        # ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºIDå¹¶ç”ŸæˆéšæœºåµŒå…¥
        words = processed_text.split()
        word_ids = []
        for word in words:
            if word not in self.vocab:
                self.vocab[word] = self.next_id
                self.next_id += 1
            word_ids.append(self.vocab[word])

        # ç”Ÿæˆæ›´å¤æ‚çš„åµŒå…¥å‘é‡
        embeddings = np.zeros(self.embedding_size)
        
        # 1. è¯çº§åˆ«åµŒå…¥
        for word_id in word_ids:
            np.random.seed(word_id)  # ä¿è¯ä¸€è‡´æ€§
            word_embedding = np.random.randn(self.embedding_size)
            embeddings += word_embedding
        
        # 2. è€ƒè™‘è¯åºä¿¡æ¯
        for i, word_id in enumerate(word_ids):
            np.random.seed(word_id + i * 1000)  # ä½ç½®ç¼–ç 
            position_embedding = np.random.randn(self.embedding_size) * 0.1
            embeddings += position_embedding
        
        # 3. è€ƒè™‘æ–‡æœ¬é•¿åº¦
        length_factor = min(len(words) / 10.0, 1.0)  # å½’ä¸€åŒ–é•¿åº¦
        embeddings *= (1 + length_factor * 0.2)
        
        # å½’ä¸€åŒ–å‘é‡
        norm = np.linalg.norm(embeddings)
        if norm > 0:
            embeddings /= norm

        # ç¼“å­˜ç»“æœ
        self.embeddings_cache[text] = embeddings
        return embeddings

    def _preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        # 1. æ ‡å‡†åŒ–
        text = text.lower()
        
        # 2. ä¿ç•™é‡è¦æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼š', ':')
        
        # 3. ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text

    def embed_documents(self, texts):
        """å®ç°Langchainæ‰€éœ€çš„æ¥å£æ–¹æ³• - æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        return [self.embed_text(text) for text in texts]

    def embed_query(self, text):
        """å®ç°Langchainæ‰€éœ€çš„æ¥å£æ–¹æ³• - åµŒå…¥æŸ¥è¯¢"""
        return self.embed_text(text)

# ===================== 1. çŸ¥è¯†åº“æ„å»ºæ¨¡å— =====================
class KnowledgeBaseBuilder:
    """æ•…éšœçŸ¥è¯†åº“æ„å»ºå™¨ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    def __init__(self, config):
        self.config = config
        # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥æ¨¡å‹
        self.embedder = CustomEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"]
        )

    def process_data(self, historical_data):
        """æ•°æ®å¤„ç†ï¼šåˆ†å—å’Œå‘é‡åŒ–"""
        documents = []
        metadatas = []

        for record in historical_data:
            # æ„å»ºæ›´ç»“æ„åŒ–çš„æ–‡æ¡£æè¿°
            structured_text = self._build_structured_document(record)
            
            # åˆ†å—å¤„ç†
            chunks = self.text_splitter.split_text(structured_text)
            
            for i, chunk in enumerate(chunks):
                documents.append(chunk)
                
                # æå–å…³é”®è¯å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                symptom_keywords = self._extract_keywords(record["symptom"])
                solution_keywords = self._extract_keywords(record["solution"])
                semantic_tags = self._generate_semantic_tags(record)
                
                metadatas.append({
                    "source": record.get("source", "unknown"),
                    "timestamp": record.get("timestamp", datetime.now().isoformat()),
                    "category": record["category"],
                    "chunk_id": str(i),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "total_chunks": str(len(chunks)),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "symptom_keywords": ", ".join(symptom_keywords) if symptom_keywords else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "solution_keywords": ", ".join(solution_keywords) if solution_keywords else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "semantic_tags": ", ".join(semantic_tags) if semantic_tags else ""  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                })

        # åˆ›å»ºå‘é‡æ•°æ®åº“ 
        print(f"åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")
        vector_db = Chroma.from_texts(
            documents, 
            metadatas=metadatas,
            embedding=self.embedder
        )
        return vector_db, documents

    def _build_structured_document(self, record):
        """æ„å»ºç»“æ„åŒ–çš„æ–‡æ¡£å†…å®¹"""
        # ä½¿ç”¨æ›´æ¸…æ™°çš„è¯­ä¹‰ç»“æ„
        structured_parts = [
            f"æ•…éšœç°è±¡: {record['symptom']}",
            f"æ•…éšœåˆ†ç±»: {record['category']}", 
            f"è§£å†³æ–¹æ¡ˆ: {record['solution']}"
        ]
        
        # æ·»åŠ è¯­ä¹‰æ ‡ç­¾
        semantic_tags = self._generate_semantic_tags(record)
        if semantic_tags:
            structured_parts.append(f"è¯­ä¹‰æ ‡ç­¾: {', '.join(semantic_tags)}")
        
        return "\n".join(structured_parts)

    def _extract_keywords(self, text):
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        important_words = ['æ¥å£', 'æˆåŠŸç‡', 'ä¸‹é™', 'è·Œè½', 'ä¾èµ–', 'ä¸‹æ¸¸', 'DB', 'æ…¢æŸ¥è¯¢', 'è¶…æ—¶', 'é”™è¯¯']
        
        for word in important_words:
            if word in text:
                keywords.append(word)
        
        return keywords

    def _generate_semantic_tags(self, record):
        """ç”Ÿæˆè¯­ä¹‰æ ‡ç­¾"""
        tags = []
        
        # åŸºäºç—‡çŠ¶ç”Ÿæˆæ ‡ç­¾
        symptom = record['symptom'].lower()
        if 'æˆåŠŸç‡' in symptom and 'ä¸‹é™' in symptom:
            tags.append('æˆåŠŸç‡å¼‚å¸¸')
        if 'ä¾èµ–' in symptom or 'ä¸‹æ¸¸' in symptom:
            tags.append('ä¾èµ–é“¾é—®é¢˜')
        if 'db' in symptom or 'æ•°æ®åº“' in symptom:
            tags.append('æ•°æ®åº“é—®é¢˜')
        if 'è¶…æ—¶' in symptom:
            tags.append('è¶…æ—¶é—®é¢˜')
        if 'å†…å­˜' in symptom:
            tags.append('å†…å­˜é—®é¢˜')
        
        return tags

# ===================== 2. ç®€åŒ–æ£€ç´¢æ¨¡å— =====================
class SimplifiedRetriever:
    """ç®€åŒ–çš„æ··åˆæ£€ç´¢å™¨"""
    def __init__(self, vector_db, documents, config):
        self.config = config
        self.vector_db = vector_db
        self.documents = documents

        # åˆ›å»ºBM25æ£€ç´¢å™¨
        print("åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")
        self.bm25_retriever = BM25Retriever.from_texts(documents)
        self.bm25_retriever.k = config["top_k"]

    def retrieve(self, query):
        """æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆé€šç”¨è¯­ä¹‰ç‰ˆæœ¬ï¼‰"""
        # 1. å‘é‡æ£€ç´¢ - ä½¿ç”¨æ›´å¤§çš„å€™é€‰é›†
        try:
            vector_results = self.vector_db.similarity_search_with_score(
                query, 
                k=self.config["top_k"] * 3  # è·å–æ›´å¤šå€™é€‰
            )
            vector_candidates = [(score, doc.page_content, doc.metadata) for doc, score in vector_results]
        except Exception as e:
            print(f"å‘é‡æ£€ç´¢é”™è¯¯: {e}")
            vector_candidates = []

        # 2. BM25æ£€ç´¢
        try:
            bm25_results = self.bm25_retriever.get_relevant_documents(query)
            bm25_candidates = [(0.5, doc.page_content, {}) for doc in bm25_results]  # é»˜è®¤åˆ†æ•°
        except Exception as e:
            print(f"BM25æ£€ç´¢é”™è¯¯: {e}")
            bm25_candidates = []

        print(f"å‘é‡æ£€ç´¢å€™é€‰: {len(vector_candidates)} ä¸ª")
        print(f"BM25æ£€ç´¢å€™é€‰: {len(bm25_candidates)} ä¸ª")

        # 3. èåˆå€™é€‰é›†
        all_candidates = self._merge_candidates(vector_candidates, bm25_candidates)
        
        # 4. é‡æ–°æ’åºå’Œå»é‡
        final_results = self._rerank_and_deduplicate(query, all_candidates)
        
        print(f"æœ€ç»ˆæ£€ç´¢åˆ° {len(final_results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        
        return final_results

    def _merge_candidates(self, vector_candidates, bm25_candidates):
        """èåˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢çš„å€™é€‰ç»“æœ"""
        merged = {}
        
        # æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
        for score, content, metadata in vector_candidates:
            if content not in merged:
                merged[content] = {
                    'score': score,
                    'metadata': metadata,
                    'sources': ['vector']
                }
        
        # æ·»åŠ BM25æ£€ç´¢ç»“æœï¼Œå¦‚æœå·²å­˜åœ¨åˆ™æå‡åˆ†æ•°
        for score, content, metadata in bm25_candidates:
            if content in merged:
                # å¦‚æœä¸¤ä¸ªæ£€ç´¢å™¨éƒ½æ‰¾åˆ°äº†ï¼Œæå‡åˆ†æ•°
                merged[content]['score'] = merged[content]['score'] * 1.2
                merged[content]['sources'].append('bm25')
            else:
                merged[content] = {
                    'score': score,
                    'metadata': metadata,
                    'sources': ['bm25']
                }
        
        return merged

    def _rerank_and_deduplicate(self, query, candidates):
        """é‡æ–°æ’åºå’Œå»é‡"""
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        candidate_list = []
        for content, info in candidates.items():
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = self._calculate_final_score(query, content, info)
            candidate_list.append((final_score, content))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        candidate_list.sort(key=lambda x: x[0], reverse=True)
        
        # å»é‡å¹¶è¿”å›top_k
        seen_contents = set()
        final_results = []
        
        for score, content in candidate_list:
            if content not in seen_contents and len(final_results) < self.config["top_k"]:
                final_results.append(content)
                seen_contents.add(content)
        
        return final_results

    def _calculate_final_score(self, query, content, info):
        """è®¡ç®—æœ€ç»ˆç›¸ä¼¼åº¦åˆ†æ•°"""
        base_score = info['score']
        
        # 1. åŸºç¡€åˆ†æ•°
        final_score = base_score
        
        # 2. å¤šæºæ£€ç´¢å¥–åŠ±ï¼ˆå¦‚æœå¤šä¸ªæ£€ç´¢å™¨éƒ½æ‰¾åˆ°äº†ï¼‰
        if len(info['sources']) > 1:
            final_score *= 1.1
        
        # 3. å…ƒæ•°æ®åŒ¹é…å¥–åŠ±
        if 'metadata' in info and info['metadata']:
            metadata = info['metadata']
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            query_lower = query.lower()
            if 'symptom_keywords' in metadata and metadata['symptom_keywords']:
                # å°†å­—ç¬¦ä¸²å…³é”®è¯åˆ†å‰²å›åˆ—è¡¨
                keywords = [kw.strip() for kw in metadata['symptom_keywords'].split(',') if kw.strip()]
                keyword_matches = sum(1 for keyword in keywords 
                                    if keyword.lower() in query_lower)
                final_score += keyword_matches * 0.1
            
            # æ£€æŸ¥è¯­ä¹‰æ ‡ç­¾åŒ¹é…ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'semantic_tags' in metadata and metadata['semantic_tags']:
                # å°†å­—ç¬¦ä¸²æ ‡ç­¾åˆ†å‰²å›åˆ—è¡¨
                tags = [tag.strip() for tag in metadata['semantic_tags'].split(',') if tag.strip()]
                tag_matches = sum(1 for tag in tags 
                                if tag.lower() in query_lower)
                final_score += tag_matches * 0.2
        
        return final_score

# ===================== 3. RAGæ¨ç†æ¨¡å—ï¼ˆé€‚é…é˜¿é‡ŒAPIï¼‰ =====================
class AliyunLLMChain:
    """å°è£…é˜¿é‡Œäº‘å¤§æ¨¡å‹è°ƒç”¨"""
    def __init__(self, config):
        self.config = config
        self.model = config["llm_model"]

    def generate(self, prompt):
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            print("è°ƒç”¨é˜¿é‡Œå¤§æ¨¡å‹API...")
            response = Generation.call(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                result_format='message'
            )
            return response["output"]["choices"][0]["message"]["content"]
        except Exception as e:
            return f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

class FaultDiagnosisSystem:
    """æ•…éšœå®šä½æ ¸å¿ƒç³»ç»Ÿ"""
    def __init__(self, retriever, config):
        self.retriever = retriever
        self.config = config
        self.llm = AliyunLLMChain(config)

        # æ›´ç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿
        self.prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è¿ç»´ä¸“å®¶ï¼Œè¯·æ ¹æ®å‘Šè­¦ä¿¡æ¯å’Œå†å²æ•…éšœæ•°æ®è¯Šæ–­é—®é¢˜ï¼š

        <å½“å‰å‘Šè­¦>
        æè¿°: {alert_desc}
        æŒ‡æ ‡: {alert_metrics}
        æ—¥å¿—: {alert_logs}

        <ç›¸å…³å†å²æ¡ˆä¾‹>
        {context}

        è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºåˆ†æç»“æœï¼š
        1. æ•…éšœç±»åˆ«: ä»å†å²æ¡ˆä¾‹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„åˆ†ç±»ï¼Œå¦‚æœå’Œå†å²æ¡ˆä¾‹çš„åˆ†ç±»åŒ¹é…åº¦ä¸é«˜ï¼Œåˆ™å¦å¤–ç»™å‡ºç®€æ´çš„æ•…éšœç±»åˆ«
        2. æ•…éšœå¯¹è±¡: å¦‚æœå®šä½åˆ°æ•…éšœæ ¹å› å¯¹è±¡ï¼Œè¯·è¾“å‡ºæ•…éšœæ ¹å› å¯¹è±¡
        3. è¯Šæ–­ä¾æ®: ç»“åˆå½“å‰å‘Šè­¦å’Œå†å²æ¡ˆä¾‹ç›¸ä¼¼ç‚¹åˆ†æ
        4. ç´§æ€¥æªæ–½: å»ºè®®çš„ç¬¬ä¸€æ­¥æ“ä½œ
        5. å¤‡æ³¨: ä»»ä½•é¢å¤–æ³¨æ„äº‹é¡¹
        """

    def build_prompt(self, alert_desc, alert_metrics, alert_logs, context):
        """æ„å»ºå®Œæ•´æç¤º"""
        # æ ¼å¼åŒ–å†å²æ¡ˆä¾‹
        context_str = "\n".join([f"ğŸ“ {doc}" for doc in context])
 
        # print("\n======= æç¤ºå†…å®¹ =======")
        # print(self.prompt_template.format(
        #     alert_desc=alert_desc,
        #     alert_metrics=alert_metrics,
        #     alert_logs=alert_logs,
        #     context=context_str
        # ))
        # print("=" * 40)

        return self.prompt_template.format(
            alert_desc=alert_desc,
            alert_metrics=alert_metrics,
            alert_logs=alert_logs,
            context=context_str
        )

    def diagnose(self, alert_desc, alert_metrics, alert_logs):
        """æ‰§è¡Œæ•…éšœè¯Šæ–­"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print(f"æ£€ç´¢å‘Šè­¦: {alert_desc}...")
        context_docs = self.retriever(alert_desc)

        # æ„å»ºæç¤º
        print("æ„å»ºæç¤º...")
        prompt = self.build_prompt(
            alert_desc, 
            alert_metrics, 
            alert_logs,
            context_docs
        )
   
        # è°ƒç”¨é˜¿é‡Œå¤§æ¨¡å‹
        return self.llm.generate(prompt)

# ===================== ä¸»æ‰§è¡Œæµç¨‹ =====================
if __name__ == "__main__":
    print("æ­£åœ¨åˆå§‹åŒ–æ•…éšœè¯Šæ–­ç³»ç»Ÿ...")

    # 0. éªŒè¯é…ç½®
    try:
        validate_config()
    except ValueError as e:
        print(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        exit(1)

    # 0.1 ç¡®ä¿é˜¿é‡ŒAPIå¯†é’¥å­˜åœ¨
    if not CONFIG["ali_api_key"]:
        print("é”™è¯¯ï¼šæœªè®¾ç½®ALI_API_KEYç¯å¢ƒå˜é‡ï¼")
        print("è¯·æ‰§è¡Œï¼šexport ALI_API_KEY=æ‚¨çš„é˜¿é‡ŒAPIå¯†é’¥")
        exit(1)

    # 1. æ„å»ºçŸ¥è¯†åº“ï¼ˆä»é…ç½®æ–‡ä»¶åŠ è½½æ•°æ®ï¼‰
    print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
    kb_builder = KnowledgeBaseBuilder(CONFIG)
    vector_db, documents = kb_builder.process_data(HISTORICAL_DATA)
    print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    # 2. åˆå§‹åŒ–æ£€ç´¢å™¨
    print("åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨...")
    retriever = SimplifiedRetriever(vector_db, documents, CONFIG)

    # 2.1 åˆ›å»ºæ£€ç´¢å‡½æ•°
    def retrieve_function(query):
        return retriever.retrieve(query)

    # 3. åˆå§‹åŒ–è¯Šæ–­ç³»ç»Ÿ
    print("åˆå§‹åŒ–è¯Šæ–­å¼•æ“...")
    diagnosis_system = FaultDiagnosisSystem(
        retrieve_function, 
        CONFIG
    )
    print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡æ¥å—è¯Šæ–­è¯·æ±‚")
    print("=" * 60)

    # 4. æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹ï¼ˆä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
    for i, test_alert in enumerate(TEST_CASES):
        print("=" * 160)
        print(f"\n===== æµ‹è¯•ç”¨ä¾‹ {i+1} =====")
        print(f"å‘Šè­¦æè¿°: {test_alert['desc']}")
        print(f"ç›¸å…³æŒ‡æ ‡: {test_alert['metrics']}")
        print(f"æ—¥å¿—ä¿¡æ¯: {test_alert['logs']}")
        if 'expected_category' in test_alert:
            print(f"é¢„æœŸåˆ†ç±»: {test_alert['expected_category']}")

        print("\næ­£åœ¨åˆ†æä¸­...")
        # æ‰§è¡Œè¯Šæ–­
        result = diagnosis_system.diagnose(
            test_alert["desc"],
            test_alert["metrics"],
            test_alert["logs"]
        )

        print("\n===== è¯Šæ–­ç»“æœ =====")
        print(result)
        print("=" * 160)

    print("æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå®Œæ¯•")
